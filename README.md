## Attention mechanisms and Transformers

* This repo is created for learning purposes.<br>
* This goal of this repo is to host basic architecture and model traning code associated with the different attention mechanisms and transformer architecture.

**Attention Mechanisms**

| **# No.** | **Mechanism**      | **Paper**                                                     |
|:---------:|:----------------------------:|:-------------------------------------------------------------:|
| 1         | [Multi-head Self Attention](https://github.com/veb-101/Attention-and-Transformers/blob/main/MobileViT-v1/multihead_self_attention_2D.py)    | [Attention is all you need](https://arxiv.org/abs/1706.03762) |
| 2         | [Multi-head Self Attention 2D](https://github.com/veb-101/Attention-and-Transformers/blob/main/MobileViT-v1/multihead_self_attention_2D.py) | [MobileViT V1](https://arxiv.org/abs/2110.02178)              |

**Transformer Models**

| **# No.** | **Models**         | **Paper**                                                          |
|:---------:|:------------------:|:------------------------------------------------------------------:|
| 1         | [Vision Transformer](https://github.com/veb-101/Attention-and-Transformers/blob/main/VisionTransformers/vision_transformer.py) | [An Image is Worth 16x16 Words:](https://arxiv.org/abs/2010.11929) |
| 2         | MobileViT - V1 (under development)     | [MobileViT V1](https://arxiv.org/abs/2110.02178)                   |
